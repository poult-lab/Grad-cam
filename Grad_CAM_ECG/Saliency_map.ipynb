{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the shape of input: torch.Size([1, 3, 224, 224])\n",
      "**************************\n",
      "Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "**************************\n",
      "Bottleneck(\n",
      "  (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      ")\n",
      "torch.Size([7, 7])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZgAAAGdCAYAAAAv9mXmAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAXH0lEQVR4nO3df2zUhf3H8Vdp1ytCewJSaMdRMKAItR1S2m+tThSUNEp0fyAhNfbLzBLJdfxoTFz/+FrNMo794b7oRiqwTfwmY8Ut3/org44xKVukUsqagCb8UAynCNUF79ounKT9fP/YvH07hfG5ft799M7nI/kk6+VzfF6XsD69+0DJchzHEQAAHhvn9wAAQGYiMAAAEwQGAGCCwAAATBAYAIAJAgMAMEFgAAAmCAwAwETOaF9waGhI586dU35+vrKyskb78gCAEXAcR319fSouLta4cVd/jzLqgTl37pxCodBoXxYA4KFoNKoZM2Zc9ZxRD0x+fr4kaePGlxUIXDfal4crk/we4K17S/1e4LmseZ/7PcFzzv/m+j3Bex/9ye8Fnkkk/qb//u+Hk9/Lr2bUA/PFx2KBwHXKy5sw2peHKxP9HuCtiQV+L/BcVkEGBmZ8BgYmA7/XXcstDm7yAwBMEBgAgAkCAwAwQWAAACYIDADABIEBAJggMAAAEwQGAGCCwAAATBAYAIAJAgMAMEFgAAAmCAwAwASBAQCYIDAAABMEBgBggsAAAEwQGACACQIDADCRUmC2bt2qWbNmKS8vT1VVVTp8+LDXuwAAac51YHbv3q3GxkY1Nzfr6NGjKi8v1/Lly9Xb22uxDwCQplwH5ic/+Ym+973vac2aNZo/f75eeOEFXXfddfrlL39psQ8AkKZcBebzzz9Xd3e3li1b9s9fYNw4LVu2TIcOHfrK5yQSCcXj8WEHACDzuQrMp59+qsHBQU2bNm3Y49OmTdP58+e/8jmRSETBYDB5hEKh1NcCANKG+Z8ia2pqUiwWSx7RaNT6kgCAMSDHzck33HCDsrOzdeHChWGPX7hwQdOnT//K5wQCAQUCgdQXAgDSkqt3MLm5uVq0aJH279+ffGxoaEj79+9XdXW15+MAAOnL1TsYSWpsbFR9fb0qKipUWVmpLVu2aGBgQGvWrLHYBwBIU64Ds2rVKn3yySd66qmndP78eX3rW9/S3r17v3TjHwDw9eY6MJLU0NCghoYGr7cAADIIP4sMAGCCwAAATBAYAIAJAgMAMEFgAAAmCAwAwASBAQCYIDAAABMEBgBggsAAAEwQGACACQIDADBBYAAAJggMAMAEgQEAmCAwAAATBAYAYILAAABMEBgAgIkcvwdgDJtV5vcCT1VV/snvCZ67TX/xe4Lnzj1W5PcEz716bqXfE7zTF5c2X9upvIMBAJggMAAAEwQGAGCCwAAATBAYAIAJAgMAMEFgAAAmCAwAwASBAQCYIDAAABMEBgBggsAAAEwQGACACQIDADBBYAAAJggMAMAEgQEAmCAwAAATBAYAYILAAABMEBgAgAkCAwAw4TowBw8e1IoVK1RcXKysrCy98sorBrMAAOnOdWAGBgZUXl6urVu3WuwBAGSIHLdPqK2tVW1trcUWAEAGcR0YtxKJhBKJRPLreDxufUkAwBhgfpM/EokoGAwmj1AoZH1JAMAYYB6YpqYmxWKx5BGNRq0vCQAYA8w/IgsEAgoEAtaXAQCMMfw9GACACdfvYPr7+3X69Onk12fOnFFPT48mT56smTNnejoOAJC+XAfmyJEjuvvuu5NfNzY2SpLq6+u1c+dOz4YBANKb68AsWbJEjuNYbAEAZBDuwQAATBAYAIAJAgMAMEFgAAAmCAwAwASBAQCYIDAAABMEBgBggsAAAEwQGACACQIDADBBYAAAJggMAMAEgQEAmCAwAAATBAYAYILAAABMEBgAgAkCAwAwkePfpQP/ODLFDX4P8Nxd9fv8nuCpm3XS7wmeK7x40e8Jnrs0Kc/vCZ5bV/ys3xM8E49f0uZrPJd3MAAAEwQGAGCCwAAATBAYAIAJAgMAMEFgAAAmCAwAwASBAQCYIDAAABMEBgBggsAAAEwQGACACQIDADBBYAAAJggMAMAEgQEAmCAwAAATBAYAYILAAABMEBgAgAkCAwAwQWAAACZcBSYSiWjx4sXKz89XYWGhHnroIZ04ccJqGwAgjbkKTEdHh8LhsDo7O7Vv3z5dvnxZ9913nwYGBqz2AQDSVI6bk/fu3Tvs6507d6qwsFDd3d369re/7ekwAEB6cxWYfxWLxSRJkydPvuI5iURCiUQi+XU8Hh/JJQEAaSLlm/xDQ0PasGGDampqVFpaesXzIpGIgsFg8giFQqleEgCQRlIOTDgc1vHjx9Xa2nrV85qamhSLxZJHNBpN9ZIAgDSS0kdkDQ0NeuONN3Tw4EHNmDHjqucGAgEFAoGUxgEA0perwDiOo+9///tqa2vTgQMHNHv2bKtdAIA05yow4XBYu3bt0quvvqr8/HydP39ekhQMBjV+/HiTgQCA9OTqHkxLS4tisZiWLFmioqKi5LF7926rfQCANOX6IzIAAK4FP4sMAGCCwAAATBAYAIAJAgMAMEFgAAAmCAwAwASBAQCYIDAAABMEBgBggsAAAEwQGACACQIDADBBYAAAJggMAMAEgQEAmCAwAAATBAYAYILAAABMuPonkz01tVIaX+Db5T2X5/cA732oQb8neKpU7/g9wXMfPO/3Au/NevRjvyd47sD/+L3AOwOXLl3zubyDAQCYIDAAABMEBgBggsAAAEwQGACACQIDADBBYAAAJggMAMAEgQEAmCAwAAATBAYAYILAAABMEBgAgAkCAwAwQWAAACYIDADABIEBAJggMAAAEwQGAGCCwAAATBAYAIAJAgMAMOEqMC0tLSorK1NBQYEKCgpUXV2tPXv2WG0DAKQxV4GZMWOGNm/erO7ubh05ckT33HOPHnzwQb3zzjtW+wAAaSrHzckrVqwY9vWPfvQjtbS0qLOzUwsWLPB0GAAgvbkKzP83ODio3/zmNxoYGFB1dfUVz0skEkokEsmv4/F4qpcEAKQR1zf5jx07pokTJyoQCOjxxx9XW1ub5s+ff8XzI5GIgsFg8giFQiMaDABID64Dc/PNN6unp0dvv/221q5dq/r6er377rtXPL+pqUmxWCx5RKPREQ0GAKQH1x+R5ebmas6cOZKkRYsWqaurS88995y2bdv2lecHAgEFAoGRrQQApJ0R/z2YoaGhYfdYAACQXL6DaWpqUm1trWbOnKm+vj7t2rVLBw4cUHt7u9U+AECachWY3t5ePfroo/r4448VDAZVVlam9vZ23XvvvVb7AABpylVgfvGLX1jtAABkGH4WGQDABIEBAJggMAAAEwQGAGCCwAAATBAYAIAJAgMAMEFgAAAmCAwAwASBAQCYIDAAABMEBgBggsAAAEwQGACACQIDADBBYAAAJggMAMAEgQEAmCAwAAATOb5d+ZM/SXkTfLs8/r33npnj9wRPvdV8u98TPDdHe/ye4L18vwd4b0kGfauLZ1/7ubyDAQCYIDAAABMEBgBggsAAAEwQGACACQIDADBBYAAAJggMAMAEgQEAmCAwAAATBAYAYILAAABMEBgAgAkCAwAwQWAAACYIDADABIEBAJggMAAAEwQGAGCCwAAATBAYAIAJAgMAMDGiwGzevFlZWVnasGGDR3MAAJki5cB0dXVp27ZtKisr83IPACBDpBSY/v5+1dXVaceOHZo0aZLXmwAAGSClwITDYd1///1atmzZvz03kUgoHo8POwAAmS/H7RNaW1t19OhRdXV1XdP5kUhEzzzzjOthAID05uodTDQa1fr16/WrX/1KeXl51/ScpqYmxWKx5BGNRlMaCgBIL67ewXR3d6u3t1e33XZb8rHBwUEdPHhQP/vZz5RIJJSdnT3sOYFAQIFAwJu1AIC04SowS5cu1bFjx4Y9tmbNGs2bN09PPvnkl+ICAPj6chWY/Px8lZaWDntswoQJmjJlypceBwB8vfE3+QEAJlz/KbJ/deDAAQ9mAAAyDe9gAAAmCAwAwASBAQCYIDAAABMEBgBggsAAAEwQGACACQIDADBBYAAAJggMAMAEgQEAmCAwAAATBAYAYILAAABMEBgAgAkCAwAwQWAAACYIDADABIEBAJjI8e/SgX8cGLvifg/w1F+eqfR7guduaj7p9wTPva45fk/wXOiJqN8TPNMf/5v0X9d2Lu9gAAAmCAwAwASBAQCYIDAAABMEBgBggsAAAEwQGACACQIDADBBYAAAJggMAMAEgQEAmCAwAAATBAYAYILAAABMEBgAgAkCAwAwQWAAACYIDADABIEBAJggMAAAEwQGAGCCwAAATLgKzNNPP62srKxhx7x586y2AQDSWI7bJyxYsEB/+MMf/vkL5Lj+JQAAXwOu65CTk6Pp06dbbAEAZBDX92BOnTql4uJi3Xjjjaqrq9PZs2even4ikVA8Hh92AAAyn6vAVFVVaefOndq7d69aWlp05swZ3Xnnnerr67vicyKRiILBYPIIhUIjHg0AGPtcBaa2tlYrV65UWVmZli9frt/97nf67LPP9PLLL1/xOU1NTYrFYskjGo2OeDQAYOwb0R3666+/XjfddJNOnz59xXMCgYACgcBILgMASEMj+nsw/f39eu+991RUVOTVHgBAhnAVmCeeeEIdHR364IMP9NZbb+k73/mOsrOztXr1aqt9AIA05eojsg8//FCrV6/WX//6V02dOlV33HGHOjs7NXXqVKt9AIA05Sowra2tVjsAABmGn0UGADBBYAAAJggMAMAEgQEAmCAwAAATBAYAYILAAABMEBgAgAkCAwAwQWAAACYIDADABIEBAJggMAAAEwQGAGCCwAAATBAYAIAJAgMAMEFgAAAmCAwAwESOf5cep8zq22W/BxjI9nuAx972e4Dndj+z2u8J3svNtN930tGc//B7gmcuXYpL+s9rOjeTvsMDAMYQAgMAMEFgAAAmCAwAwASBAQCYIDAAABMEBgBggsAAAEwQGACACQIDADBBYAAAJggMAMAEgQEAmCAwAAATBAYAYILAAABMEBgAgAkCAwAwQWAAACYIDADABIEBAJggMAAAE64D89FHH+mRRx7RlClTNH78eN166606cuSIxTYAQBrLcXPyxYsXVVNTo7vvvlt79uzR1KlTderUKU2aNMlqHwAgTbkKzI9//GOFQiG9+OKLycdmz57t+SgAQPpz9RHZa6+9poqKCq1cuVKFhYVauHChduzYcdXnJBIJxePxYQcAIPO5Csz777+vlpYWzZ07V+3t7Vq7dq3WrVunl1566YrPiUQiCgaDySMUCo14NABg7MtyHMe51pNzc3NVUVGht956K/nYunXr1NXVpUOHDn3lcxKJhBKJRPLreDyuUCikH/xgv/LyJoxg+lhz2e8BBqb6PcBjn/k9wECF3wO8l5vt9wLvuboZMbZduhTX5qeCisViKigouOq5rt7BFBUVaf78+cMeu+WWW3T27NkrPicQCKigoGDYAQDIfK4CU1NToxMnTgx77OTJkyopKfF0FAAg/bkKzMaNG9XZ2alNmzbp9OnT2rVrl7Zv365wOGy1DwCQplwFZvHixWpra9Ovf/1rlZaW6oc//KG2bNmiuro6q30AgDTl+tbTAw88oAceeMBiCwAgg/CzyAAAJggMAMAEgQEAmCAwAAATBAYAYILAAABMEBgAgAkCAwAwQWAAACYIDADABIEBAJggMAAAEwQGAGCCwAAATBAYAIAJAgMAMEFgAAAmCAwAwITrfzJ5pBzHkSQlEgOjfWljl/0eYGC83wM8lmm/5yQp7vcA7w1l+73Ae6P+ndZO4tLff8998b38arKcaznLQx9++KFCodBoXhIA4LFoNKoZM2Zc9ZxRD8zQ0JDOnTun/Px8ZWVlmV0nHo8rFAopGo2qoKDA7Dqjidc09mXa65F4TelitF6T4zjq6+tTcXGxxo27+l2WUX/jNm7cuH9bPS8VFBRkzG+gL/Caxr5Mez0SryldjMZrCgaD13QeN/kBACYIDADARMYGJhAIqLm5WYFAwO8pnuE1jX2Z9nokXlO6GIuvadRv8gMAvh4y9h0MAMBfBAYAYILAAABMEBgAgImMDMzWrVs1a9Ys5eXlqaqqSocPH/Z70ogcPHhQK1asUHFxsbKysvTKK6/4PWlEIpGIFi9erPz8fBUWFuqhhx7SiRMn/J41Ii0tLSorK0v+Jbfq6mrt2bPH71me2rx5s7KysrRhwwa/p6Ts6aefVlZW1rBj3rx5fs8akY8++kiPPPKIpkyZovHjx+vWW2/VkSNH/J4lKQMDs3v3bjU2Nqq5uVlHjx5VeXm5li9frt7eXr+npWxgYEDl5eXaunWr31M80dHRoXA4rM7OTu3bt0+XL1/Wfffdp4GB9P1hlDNmzNDmzZvV3d2tI0eO6J577tGDDz6od955x+9pnujq6tK2bdtUVlbm95QRW7BggT7++OPk8ec//9nvSSm7ePGiampq9I1vfEN79uzRu+++q2effVaTJk3ye9rfORmmsrLSCYfDya8HBwed4uJiJxKJ+LjKO5KctrY2v2d4qre315HkdHR0+D3FU5MmTXJ+/vOf+z1jxPr6+py5c+c6+/btc+666y5n/fr1fk9KWXNzs1NeXu73DM88+eSTzh133OH3jCvKqHcwn3/+ubq7u7Vs2bLkY+PGjdOyZct06NAhH5fhamKxmCRp8uTJPi/xxuDgoFpbWzUwMKDq6mq/54xYOBzW/fffP+z/V+ns1KlTKi4u1o033qi6ujqdPXvW70kpe+2111RRUaGVK1eqsLBQCxcu1I4dO/yelZRRgfn00081ODioadOmDXt82rRpOn/+vE+rcDVDQ0PasGGDampqVFpa6vecETl27JgmTpyoQCCgxx9/XG1tbZo/f77fs0aktbVVR48eVSQS8XuKJ6qqqrRz507t3btXLS0tOnPmjO6880719fX5PS0l77//vlpaWjR37ly1t7dr7dq1WrdunV566SW/p0nKqH8GB+koHA7r+PHjaf05+Bduvvlm9fT0KBaL6be//a3q6+vV0dGRtpGJRqNav3699u3bp7y8PL/neKK2tjb5v8vKylRVVaWSkhK9/PLLeuyxx3xclpqhoSFVVFRo06ZNkqSFCxfq+PHjeuGFF1RfX+/zugx7B3PDDTcoOztbFy5cGPb4hQsXNH36dJ9W4UoaGhr0xhtv6M033xzVf8LBSm5urubMmaNFixYpEomovLxczz33nN+zUtbd3a3e3l7ddtttysnJUU5Ojjo6OvT8888rJydHg4ODfk8cseuvv1433XSTTp8+7feUlBQVFX3pP2BuueWWMfOxX0YFJjc3V4sWLdL+/fuTjw0NDWn//v0Z8Vl4pnAcRw0NDWpra9Mf//hHzZ492+9JJoaGhpRIJPyekbKlS5fq2LFj6unpSR4VFRWqq6tTT0+PsrPT/5827u/v13vvvaeioiK/p6SkpqbmS3/E/+TJkyopKfFp0XAZ9xFZY2Oj6uvrVVFRocrKSm3ZskUDAwNas2aN39NS1t/fP+y/sM6cOaOenh5NnjxZM2fO9HFZasLhsHbt2qVXX31V+fn5yftjwWBQ48eP93ldapqamlRbW6uZM2eqr69Pu3bt0oEDB9Te3u73tJTl5+d/6b7YhAkTNGXKlLS9X/bEE09oxYoVKikp0blz59Tc3Kzs7GytXr3a72kp2bhxo26//XZt2rRJDz/8sA4fPqzt27dr+/btfk/7O7//GJuFn/70p87MmTOd3Nxcp7Ky0uns7PR70oi8+eabjqQvHfX19X5PS8lXvRZJzosvvuj3tJR997vfdUpKSpzc3Fxn6tSpztKlS53f//73fs/yXLr/MeVVq1Y5RUVFTm5urvPNb37TWbVqlXP69Gm/Z43I66+/7pSWljqBQMCZN2+es337dr8nJfHj+gEAJjLqHgwAYOwgMAAAEwQGAGCCwAAATBAYAIAJAgMAMEFgAAAmCAwAwASBAQCYIDAAABMEBgBggsAAAEz8H1R4Ja1zBcf9AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import ResNet50_Weights\n",
    "\n",
    "\n",
    "# Load the image\n",
    "image_path = 'bird.jpg'\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# Preprocess the image\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "input_tensor = preprocess(image)\n",
    "input_batch = input_tensor.unsqueeze(0)  # Create a mini-batch as expected by the model\n",
    "\n",
    "# Check if a GPU is available and move the input and model to GPU\n",
    "device = torch.device('cpu')\n",
    "model = models.resnet50(weights = ResNet50_Weights.IMAGENET1K_V1)\n",
    "model = model.to(device)\n",
    "input_batch = input_batch.to(device)\n",
    "print(\"This is the shape of input:\", input_batch.shape)\n",
    "\n",
    "\n",
    "\n",
    "def generate_gradcam(model, input_tensor, target_layer):\n",
    "    # Register hook to the target layer\n",
    "    gradients = []\n",
    "    activations = []\n",
    "\n",
    "    def forward_hook(module, input, output):\n",
    "        activations.append(output)\n",
    "    \n",
    "    def backward_hook(module, grad_in, grad_out):\n",
    "        gradients.append(grad_out[0])\n",
    "    \n",
    "    hook_handles = []\n",
    "    hook_handles.append(target_layer.register_forward_hook(forward_hook))\n",
    "    hook_handles.append(target_layer.register_full_backward_hook(backward_hook))\n",
    "\n",
    "    # Forward pass\n",
    "    model.eval()\n",
    "    output = model(input_tensor)\n",
    "\n",
    "    # Get the index of the class with the highest score\n",
    "    _, predicted_class = output.max(1)\n",
    "\n",
    "    # Zero gradients\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Backward pass\n",
    "    output[:, predicted_class].backward()\n",
    "\n",
    "    # Get hooked activations and gradients\n",
    "    activations = activations[0].detach()\n",
    "    gradients = gradients[0].detach()\n",
    "\n",
    "    # Compute the weight coefficients\n",
    "    pooled_gradients = torch.mean(gradients, dim=[0, 2, 3])\n",
    "\n",
    "    # Weight the channels of the activations\n",
    "    for i in range(activations.size(1)):\n",
    "        activations[:, i, :, :] *= pooled_gradients[i]\n",
    "    \n",
    "    # Compute the Grad-CAM\n",
    "    gradcam = torch.mean(activations, dim=1).squeeze()\n",
    "\n",
    "    # Apply ReLU\n",
    "    gradcam = F.relu(gradcam)\n",
    "\n",
    "    # Normalize the Grad-CAM\n",
    "    gradcam = gradcam - gradcam.min()\n",
    "    gradcam = gradcam / gradcam.max()\n",
    "    \n",
    "    # Remove hooks\n",
    "    for handle in hook_handles:\n",
    "        handle.remove()\n",
    "    \n",
    "    return gradcam\n",
    "\n",
    "# Specify the target layer\n",
    "target_layer = model.layer4[2].conv3  # This may change depending on the architecture\n",
    "block = model.layer4[2]\n",
    "\n",
    "# print the architecture of model\n",
    "# print(model)\n",
    "print(\"**************************\")\n",
    "print(target_layer)\n",
    "print(\"**************************\")\n",
    "print(block)\n",
    "\n",
    "# Generate Grad-CAM\n",
    "gradcam = generate_gradcam(model, input_batch, target_layer)\n",
    "print(gradcam.shape)\n",
    "# Visualize the Grad-CAM\n",
    "plt.imshow(gradcam.cpu(), cmap='jet', alpha=0.5)\n",
    "plt.axis('on')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.95951656 0.19327376 0.26351115 0.08236055 0.62689737 0.07911348\n",
      "  0.94882022 0.1014926  0.00262009 0.08316236]\n",
      " [0.23138782 0.62438972 0.08062522 0.75188433 0.42504074 0.94537676\n",
      "  0.29897507 0.35599303 0.64051686 0.13525616]\n",
      " [0.90444237 0.05256651 0.61860994 0.54312472 0.07071008 0.96629768\n",
      "  0.89357251 0.39730931 0.43849018 0.42499534]\n",
      " [0.28147068 0.51137108 0.97110375 0.76457509 0.89404678 0.23549352\n",
      "  0.50867152 0.00756577 0.19676195 0.76717641]\n",
      " [0.56946602 0.15894756 0.66024553 0.9239829  0.58243817 0.54782615\n",
      "  0.01080361 0.96222159 0.85031268 0.23077987]\n",
      " [0.64955572 0.94370774 0.60038999 0.28645753 0.58840939 0.4701122\n",
      "  0.43040258 0.69523582 0.13688974 0.79872093]\n",
      " [0.27921767 0.52532063 0.37939519 0.78480115 0.70932656 0.87057289\n",
      "  0.39110152 0.18908035 0.72067206 0.4441533 ]\n",
      " [0.84957803 0.16752054 0.09228551 0.83454904 0.91326057 0.27604445\n",
      "  0.43354345 0.87864182 0.50839583 0.1454071 ]\n",
      " [0.47192969 0.94318301 0.74845394 0.95278029 0.65494371 0.6205994\n",
      "  0.11514173 0.75393286 0.56032192 0.03080312]\n",
      " [0.67938445 0.63834735 0.5839821  0.32372172 0.27789284 0.96534975\n",
      "  0.89578624 0.16059203 0.31930494 0.53734632]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Create a 2D array (for example, an image with random values)\n",
    "image_data = np.random.rand(10, 10)\n",
    "print(image_data)\n",
    "# Display the image\n",
    "plt.imshow(image_data)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1200x1197 at 0x726A2895B550>\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Load the image\n",
    "image_path = 'golden_retriver.jpg'\n",
    "image = Image.open(image_path)\n",
    "print(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Format: JPEG\n",
      "Size: (1200, 1197)\n",
      "Mode: RGB\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Open an image file\n",
    "img = Image.open(\"golden_retriver.jpg\")\n",
    "\n",
    "# Display the image\n",
    "img.show()\n",
    "\n",
    "# Get basic information about the image\n",
    "print(f\"Format: {img.format}\")  # Output: JPEG, PNG, etc.\n",
    "print(f\"Size: {img.size}\")      # Output: (width, height)\n",
    "print(f\"Mode: {img.mode}\")      # Output: \"RGB\", \"L\" (grayscale), etc.\n",
    "\n",
    "# # Save the image in a different format\n",
    "# img.save(\"path_to_save_image.png\")\n",
    "\n",
    "# Resize the image\n",
    "img_resized = img.resize((100, 100))\n",
    "img_resized.show()\n",
    "\n",
    "# Convert the image to grayscale\n",
    "img_gray = img.convert(\"L\")\n",
    "img_gray.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import ResNet50_Weights\n",
    "\n",
    "\n",
    "# Load the image\n",
    "image_path = 'golden_retriver.jpg'\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# Preprocess the image\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "input_tensor = preprocess(image)\n",
    "print(input_tensor.shape)\n",
    "input_batch = input_tensor.unsqueeze(0)  # Create a mini-batch as expected by the model\n",
    "print(input_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])) is deprecated. Please ensure they have the same size.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m output \u001b[38;5;241m=\u001b[39m model(data)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Backward pass to compute gradients\u001b[39;00m\n\u001b[1;32m     33\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba/lib/python3.10/site-packages/torch/nn/modules/loss.py:618\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    617\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 618\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba/lib/python3.10/site-packages/torch/nn/functional.py:3118\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3116\u001b[0m     reduction_enum \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction)\n\u001b[1;32m   3117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize():\n\u001b[0;32m-> 3118\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3119\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing a target size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) that is different to the input size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) is deprecated. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3120\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure they have the same size.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(target\u001b[38;5;241m.\u001b[39msize(), \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m   3121\u001b[0m     )\n\u001b[1;32m   3123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3124\u001b[0m     new_size \u001b[38;5;241m=\u001b[39m _infer_size(target\u001b[38;5;241m.\u001b[39msize(), weight\u001b[38;5;241m.\u001b[39msize())\n",
      "\u001b[0;31mValueError\u001b[0m: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])) is deprecated. Please ensure they have the same size."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Example one-dimensional data (e.g., a time series)\n",
    "data = torch.tensor([[0.1, 0.2, 0.3, 0.4, 0.5]], requires_grad=True)  # 1D input data\n",
    "target = torch.tensor([1])  # Example target label for classification\n",
    "\n",
    "# Define a simple neural network\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(5, 3)\n",
    "        self.fc2 = nn.Linear(3, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = SimpleNN()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Forward pass\n",
    "output = model(data)\n",
    "\n",
    "# Compute loss\n",
    "loss = criterion(output, target.float())\n",
    "\n",
    "# Backward pass to compute gradients\n",
    "loss.backward()\n",
    "\n",
    "# The gradient of the loss with respect to the input data\n",
    "saliency = data.grad.data.abs()\n",
    "\n",
    "print(\"Input Data: \", data)\n",
    "print(\"Saliency Map: \", saliency)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vim3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
